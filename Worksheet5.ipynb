{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f9dfd8c0-944c-43c1-a218-d07a786d4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Implementation from Scratch Step - by - Step Guide:\n",
    "# 3.1.1 Step -1- Data Understanding, Analysis and Preparations:\n",
    "# In this step we will read the data, understand the data, perform some basic data cleaning, and store everything\n",
    "# in the matrix as shown below.\n",
    "# • Requirements:\n",
    "# Dataset → student.csv\n",
    "# • Decision Process:\n",
    "# In this step we will define the objective of the task.\n",
    "# – Objective of the Task -\n",
    "# To Predict the marks obtained in writing based on the marks of Math and Reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72af4cff-4be0-44f7-8c97-b4b188a7695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Rows:\n",
      "   Math  Reading  Writing\n",
      "0    48       68       63\n",
      "1    62       81       72\n",
      "2    79       80       78\n",
      "3    76       83       79\n",
      "4    59       64       62\n",
      "\n",
      "Last 5 Rows:\n",
      "     Math  Reading  Writing\n",
      "995    72       74       70\n",
      "996    73       86       90\n",
      "997    89       87       94\n",
      "998    83       82       78\n",
      "999    66       66       72\n"
     ]
    }
   ],
   "source": [
    "# • To - Do - 1:\n",
    "# 1. Read and Observe the Dataset.\n",
    "# 2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
    "# 3. Print the Information of Datasets. {Hint: pd.info}.\n",
    "# 4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
    "# 5. Split your data into Feature (X) and Label (Y).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read the dataset\n",
    "df = pd.read_csv(\"student.csv\")\n",
    "\n",
    "# 2. Print top 5 and bottom 5 rows\n",
    "\n",
    "print(\"First 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nLast 5 Rows:\")\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af444dff-c816-40ce-868e-e594969095da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Math     1000 non-null   int64\n",
      " 1   Reading  1000 non-null   int64\n",
      " 2   Writing  1000 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 3. Print dataset information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee52cc62-d1c1-481a-bde2-5a1e12647149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Math      Reading      Writing\n",
      "count  1000.000000  1000.000000  1000.000000\n",
      "mean     67.290000    69.872000    68.616000\n",
      "std      15.085008    14.657027    15.241287\n",
      "min      13.000000    19.000000    14.000000\n",
      "25%      58.000000    60.750000    58.000000\n",
      "50%      68.000000    70.000000    69.500000\n",
      "75%      78.000000    81.000000    79.000000\n",
      "max     100.000000   100.000000   100.000000\n"
     ]
    }
   ],
   "source": [
    "# 4. Print descriptive statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "303aa84e-23a3-4e8b-8556-6c693e252c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features (X):\n",
      "   Math  Reading\n",
      "0    48       68\n",
      "1    62       81\n",
      "2    79       80\n",
      "3    76       83\n",
      "4    59       64\n",
      "\n",
      "Label (Y):\n",
      "0    63\n",
      "1    72\n",
      "2    78\n",
      "3    79\n",
      "4    62\n",
      "Name: Writing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Split into Features (X) and Label (Y)\n",
    "X = df[['Math', 'Reading']]\n",
    "Y = df['Writing']\n",
    "\n",
    "print(\"\\nFeatures (X):\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nLabel (Y):\")\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c04da733-aa40-4eda-872f-e095ef0df804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix X shape: (1000, 2)\n",
      "Weight Matrix W shape: (2, 1)\n",
      "Label Matrix Y shape: (1000, 1)\n",
      "\n",
      "Predicted Output (first 5 values):\n",
      "[[ 69.43815261]\n",
      " [ 87.48819585]\n",
      " [103.99516596]\n",
      " [101.99234679]\n",
      " [ 79.03829979]]\n"
     ]
    }
   ],
   "source": [
    "# • To - Do - 2:\n",
    "# 1. To make the task easier - let’s assume there is no bias or intercept.\n",
    "# 2. Create the following matrices:\n",
    "\n",
    "# Y = WTX\n",
    "\n",
    "# 3. Note: The feature matrix described above does not include a column of 1s, as it assumes the\n",
    "# absence of a bias term in the model.\n",
    "\n",
    "\n",
    "\n",
    "# Feature matrix (X)\n",
    "X = df[['Math', 'Reading']].values   # shape (n, d)\n",
    "\n",
    "# Label vector (Y)\n",
    "Y = df['Writing'].values.reshape(-1, 1)  # shape (n, 1)\n",
    "\n",
    "# Number of features\n",
    "d = X.shape[1]\n",
    "\n",
    "# Initialize weight vector W (random)\n",
    "W = np.random.rand(d, 1)\n",
    "\n",
    "# Prediction using matrix form\n",
    "# Y_pred = XW\n",
    "Y_pred = np.dot(X, W)\n",
    "\n",
    "print(\"Feature Matrix X shape:\", X.shape)\n",
    "print(\"Weight Matrix W shape:\", W.shape)\n",
    "print(\"Label Matrix Y shape:\", Y.shape)\n",
    "print(\"\\nPredicted Output (first 5 values):\")\n",
    "print(Y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc5c6dca-c8a8-4988-ba8b-a4e02ba9be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (800, 2) (800, 1)\n",
      "Testing Data Shape: (200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# • To Do - 3:\n",
    "# 1. Split the dataset into training and test sets.\n",
    "# 2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
    "# for testing.\n",
    "\n",
    "\n",
    "# Set split ratio (80-20)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.random.permutation(len(X))\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "# Train-Test split\n",
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "\n",
    "Y_train = Y[:split_index]\n",
    "Y_test = Y[split_index:]\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "005feb6c-5c58-4fce-869f-bc625fbb5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -4\n",
    "# Feel free to build your own code or complete the following code:\n",
    "# Building a Cost Function:\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    This function finds the Mean Squared Error (MSE).\n",
    "\n",
    "    Parameters:\n",
    "    X : Feature Matrix (n x d)\n",
    "    Y : Target Matrix (n x 1)\n",
    "    W : Weight Matrix (d x 1)\n",
    "\n",
    "    Returns:\n",
    "    cost : Mean Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    W = W.reshape(-1, 1)\n",
    "\n",
    "    # Number of training examples\n",
    "    n = len(Y)\n",
    "\n",
    "    # Predicted output\n",
    "    Y_pred = np.dot(X, W)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    cost = (1 / n) * np.sum((Y_pred - Y) ** 2)\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "00c4a1bb-54a4-4824-a855-e1ae0029b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed Further\n",
      "Cost function output: 510.6554894909655\n"
     ]
    }
   ],
   "source": [
    "#TODO 5\n",
    "# Make sure your code at To - Do - 4 passed the following test case:\n",
    "# Testing a Cost Function:\n",
    "\n",
    "# Test Example (should output 0)\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "Y_test = np.array([3, 7, 11])\n",
    "W_test = np.array([1, 1])\n",
    "\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "\n",
    "if cost == 0:\n",
    "    print(\"Proceed Further\")\n",
    "else:\n",
    "    print(\"Something went wrong!\")\n",
    "\n",
    "print(\"Cost function output:\", cost_function(X, Y, W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8c33487c-3b69-423c-a8ef-42203cd9d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 6\n",
    "# Implement your code for Gradient Descent; Either fill the following code or write your own:\n",
    "# Gradient Descent from Scratch:\n",
    "\n",
    "def gradient_descent(X, Y, W, alpha=0.01, iterations=10):\n",
    "  #Ensure inputs are numpay array and coorect shape\n",
    "  X = np.array(X,dtype=float)\n",
    "  Y= np.array(Y,dtype=float).reshape(-1,1)\n",
    "  W= np.array(W,dtype=float).reshape(-1,1)\n",
    "\n",
    "  m= len(Y)\n",
    "  cost_history = [0] * iterations  # To store cost at each iteration\n",
    "  W_update = W.copy()\n",
    "\n",
    "  for iteration in range(iterations):\n",
    "    # Step 1: Hypothesis values\n",
    "      Y_pred = X @ W_update\n",
    "    # Step 2: Difference between hypothesis and actual Y\n",
    "      loss = Y_pred - Y\n",
    "    # Step 3: Gradient calculation\n",
    "      dw = (1/m) * (X.T @ loss)\n",
    "\n",
    "    # Step 4: Update W\n",
    "      W_update = W_update - alpha * dw\n",
    "\n",
    "    # Step 5: Compute new cost\n",
    "      # Cost\n",
    "      cost = cost_function(X, Y, W_update)\n",
    "      cost_history.append(cost)\n",
    "    # PRINT one line per iteration\n",
    "      print(f\"Iteration {iteration+1}:\")\n",
    "      print(\"  Weights:\\n\", W_update)\n",
    "      print(\"  Cost:\", cost)\n",
    "      print(\"-\" * 30)\n",
    "  return W_update, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4cc26335-8e6b-4e95-809f-9ddb6744eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "  Weights:\n",
      " [[0.3996496 ]\n",
      " [0.92745322]\n",
      " [0.09826523]]\n",
      "  Cost: 0.21422394189320307\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[0.39805338]\n",
      " [0.92562822]\n",
      " [0.09692923]]\n",
      "  Cost: 0.21269761199879803\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[0.39647071]\n",
      " [0.9238163 ]\n",
      " [0.0956068 ]]\n",
      "  Cost: 0.21119652631361235\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[0.39490147]\n",
      " [0.92201736]\n",
      " [0.09429783]]\n",
      "  Cost: 0.20972025896641117\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[0.39334557]\n",
      " [0.92023128]\n",
      " [0.09300221]]\n",
      "  Cost: 0.2082683912857068\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[0.39180287]\n",
      " [0.91845795]\n",
      " [0.09171981]]\n",
      "  Cost: 0.2068405116780125\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[0.39027327]\n",
      " [0.91669728]\n",
      " [0.09045053]]\n",
      "  Cost: 0.2054362155081552\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[0.38875666]\n",
      " [0.91494916]\n",
      " [0.08919425]]\n",
      "  Cost: 0.2040551049816124\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[0.38725292]\n",
      " [0.91321348]\n",
      " [0.08795085]]\n",
      "  Cost: 0.20269678902883864\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[0.38576196]\n",
      " [0.91149014]\n",
      " [0.08672023]]\n",
      "  Cost: 0.2013608831915474\n",
      "------------------------------\n",
      "Final Parameters:\n",
      "[[0.38576196]\n",
      " [0.91149014]\n",
      " [0.08672023]]\n",
      "\n",
      "Final Cost: 0.2013608831915474\n",
      "Cost History Length: 20\n"
     ]
    }
   ],
   "source": [
    "#TODO 7 \n",
    "# Make sure following Test Case is passe by your code from To - Do - 6 or your Gradient Descent Implementation:\n",
    "# Test Code for Gradient Descent function:\n",
    "\n",
    "# Generate random test data\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X = np.random.rand(100, 3)   # 100 samples, 3 features\n",
    "Y = np.random.rand(100)\n",
    "W = np.random.rand(3)        # Initial guess\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 10\n",
    "\n",
    "# Run Gradient Descent\n",
    "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "\n",
    "print(\"Final Parameters:\")\n",
    "print(final_params)\n",
    "\n",
    "print(\"\\nFinal Cost:\", cost_history[-1])\n",
    "print(\"Cost History Length:\", len(cost_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1727d10a-a9f8-47c9-a40d-dc44da8457ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 8\n",
    "# Implementation of RMSE in the Code - Complete the following code or write your own:\n",
    "# Code for RMSE:\n",
    "\n",
    "\n",
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the Root Mean Squared Error (RMSE).\n",
    "\n",
    "    Input Arguments:\n",
    "    Y      : Array of actual (target) dependent variables\n",
    "    Y_pred : Array of predicted dependent variables\n",
    "\n",
    "    Output Arguments:\n",
    "    rmse   : Root Mean Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure proper shapes\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 1)\n",
    "\n",
    "    # RMSE calculation\n",
    "    rmse_value = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
    "\n",
    "    return rmse_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c231668e-84ac-4bf1-b0b9-59ef02a5a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 9\n",
    "# Complete the following code or write your own for r2 loss:\n",
    "# Code for R-Squared Error:\n",
    "\n",
    "\n",
    "# Model Evaluation - R2\n",
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the R Squared Error.\n",
    "\n",
    "    Input Arguments:\n",
    "    Y      : Array of actual (target) dependent variables\n",
    "    Y_pred : Array of predicted dependent variables\n",
    "\n",
    "    Output Arguments:\n",
    "    r2     : R Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure proper shapes\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 1)\n",
    "\n",
    "    # Mean of actual values\n",
    "    mean_y = np.mean(Y)\n",
    "\n",
    "    # Total Sum of Squares (SS_tot)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "\n",
    "    # Residual Sum of Squares (SS_res)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "\n",
    "    # R-squared calculation\n",
    "    r2_value = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return r2_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56c64c01-f14e-497f-ade5-87e03428c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "  Weights:\n",
      " [[0.48033663]\n",
      " [0.49891288]]\n",
      "  Cost: 35.916694950829864\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[0.48805486]\n",
      " [0.51025007]]\n",
      "  Cost: 33.94509242319125\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[0.48655881]\n",
      " [0.51206745]]\n",
      "  Cost: 33.83518317899331\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[0.4848899 ]\n",
      " [0.51369237]]\n",
      "  Cost: 33.72689862161107\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[0.48322451]\n",
      " [0.51530686]]\n",
      "  Cost: 33.619522977208575\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[0.48156605]\n",
      " [0.51691449]]\n",
      "  Cost: 33.51304835155669\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[0.47991457]\n",
      " [0.51851536]]\n",
      "  Cost: 33.40746718385679\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[0.47827003]\n",
      " [0.5201095 ]]\n",
      "  Cost: 33.30277197685672\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[0.4766324 ]\n",
      " [0.52169693]]\n",
      "  Cost: 33.19895529621589\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[0.47500166]\n",
      " [0.52327769]]\n",
      "  Cost: 33.09600976997729\n",
      "------------------------------\n",
      "Final Weights:\n",
      " [[0.47500166]\n",
      " [0.52327769]]\n",
      "\n",
      "Cost History (First 10 Iterations):\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "RMSE on Test Set: 5.912415997779032\n",
      "R-Squared on Test Set: 0.8341956342436562\n"
     ]
    }
   ],
   "source": [
    "#TODO 10\n",
    "# We will define a function that:\n",
    "# 1. Loads the data and splits it into training and test sets.\n",
    "# 2. Prepares the feature matrix (X) and target vector (Y).\n",
    "# 3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
    "# 4. Calls the gradient descent function to learn the parameters.\n",
    "# 5. Evaluates the model using RMSE and R2\n",
    "# Re-write the following code or Write your own:\n",
    "\n",
    "# Compiling everything:\n",
    "\n",
    "\n",
    "#  Main Function\n",
    "def main():\n",
    "\n",
    "    # Step 1: Load dataset\n",
    "    data = pd.read_csv(\"student.csv\")\n",
    "\n",
    "    # Step 2: Prepare features (X) and target (Y)\n",
    "    X = data[['Math', 'Reading']].values\n",
    "    Y = data['Writing'].values\n",
    "\n",
    "    # Step 3: Train-Test Split (80-20)\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    split_index = int(0.8 * len(X))\n",
    "\n",
    "    train_idx = indices[:split_index]\n",
    "    test_idx = indices[split_index:]\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    # Step 4: Initialize weights and hyperparameters\n",
    "    W = np.zeros(X_train.shape[1])\n",
    "    alpha = 0.0001\n",
    "    iterations = 10\n",
    "\n",
    "    # Step 5: Train using Gradient Descent\n",
    "    W_optimal, cost_history = gradient_descent(\n",
    "        X_train, Y_train, W, alpha, iterations\n",
    "    )\n",
    "\n",
    "    # Step 6: Make predictions on test data\n",
    "    Y_pred = np.dot(X_test, W_optimal)\n",
    "\n",
    "    # Step 7: Evaluate model\n",
    "    model_rmse = rmse(Y_test, Y_pred)\n",
    "    model_r2 = r2(Y_test, Y_pred)\n",
    "\n",
    "    # Step 8: Output results\n",
    "    print(\"Final Weights:\\n\", W_optimal)\n",
    "    print(\"\\nCost History (First 10 Iterations):\\n\", cost_history[:10])\n",
    "    print(\"\\nRMSE on Test Set:\", model_rmse)\n",
    "    print(\"R-Squared on Test Set:\", model_r2)\n",
    "\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a46263d2-ad37-4b8b-8c26-89d5a716f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL FINDINGS\n",
      "\n",
      "Step 1: Loading dataset (student.csv)\n",
      "Dataset loaded successfully.\n",
      "Features: Math & Reading\n",
      "Target: Writing\n",
      "\n",
      "Step 2: Splitting dataset into Training (80%) and Testing (20%)\n",
      "Training samples: 800\n",
      "Testing samples: 200 \n",
      "\n",
      "Step 3: Experimenting with different learning rates\n",
      "Purpose: Observe convergence speed and model stability\n",
      "\n",
      "--- Learning Rate: 1e-05 ---\n",
      "Iteration 1:\n",
      "  Weights:\n",
      " [[0.04803366]\n",
      " [0.04989129]]\n",
      "  Cost: 4013.6789601666273\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[0.09134114]\n",
      " [0.09490682]]\n",
      "  Cost: 3271.549326657264\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[0.13038584]\n",
      " [0.13552465]]\n",
      "  Cost: 2667.813093201697\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[0.16558573]\n",
      " [0.17217596]]\n",
      "  Cost: 2176.662076631642\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[0.19731779]\n",
      " [0.20524966]]\n",
      "  Cost: 1777.1009126182087\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[0.22592205]\n",
      " [0.23509652]]\n",
      "  Cost: 1452.0495409171074\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[0.25170519]\n",
      " [0.26203293]]\n",
      "  Cost: 1187.613062950526\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[0.27494383]\n",
      " [0.28634424]]\n",
      "  Cost: 972.487759426439\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[0.29588746]\n",
      " [0.30828783]]\n",
      "  Cost: 797.4778762973347\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[0.31476111]\n",
      " [0.32809586]]\n",
      "  Cost: 655.1025225112732\n",
      "------------------------------\n",
      "Training RMSE: 25.5950\n",
      "Testing RMSE:  26.0675\n",
      "Testing R²:    -2.2230\n",
      "Explanation:\n",
      "- Training error is lower than testing error.\n",
      "- This is expected and indicates NO overfitting.\n",
      "- Model performance is acceptable.\n",
      "\n",
      "Learning Rate Observation:\n",
      "- Very small learning rate.\n",
      "- Converges slowly but stable.\n",
      "\n",
      "--------------------------------------------------\n",
      "--- Learning Rate: 0.0001 ---\n",
      "Iteration 1:\n",
      "  Weights:\n",
      " [[0.48033663]\n",
      " [0.49891288]]\n",
      "  Cost: 35.916694950829864\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[0.48805486]\n",
      " [0.51025007]]\n",
      "  Cost: 33.94509242319125\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[0.48655881]\n",
      " [0.51206745]]\n",
      "  Cost: 33.83518317899331\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[0.4848899 ]\n",
      " [0.51369237]]\n",
      "  Cost: 33.72689862161107\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[0.48322451]\n",
      " [0.51530686]]\n",
      "  Cost: 33.619522977208575\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[0.48156605]\n",
      " [0.51691449]]\n",
      "  Cost: 33.51304835155669\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[0.47991457]\n",
      " [0.51851536]]\n",
      "  Cost: 33.40746718385679\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[0.47827003]\n",
      " [0.5201095 ]]\n",
      "  Cost: 33.30277197685672\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[0.4766324 ]\n",
      " [0.52169693]]\n",
      "  Cost: 33.19895529621589\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[0.47500166]\n",
      " [0.52327769]]\n",
      "  Cost: 33.09600976997729\n",
      "------------------------------\n",
      "Training RMSE: 5.7529\n",
      "Testing RMSE:  5.9124\n",
      "Testing R²:    0.8342\n",
      "Explanation:\n",
      "- Training error is lower than testing error.\n",
      "- This is expected and indicates NO overfitting.\n",
      "- Model performance is acceptable.\n",
      "\n",
      "Learning Rate Observation:\n",
      "- Balanced learning rate.\n",
      "- Faster convergence with stability.\n",
      "\n",
      "--------------------------------------------------\n",
      "--- Learning Rate: 0.001 ---\n",
      "Iteration 1:\n",
      "  Weights:\n",
      " [[4.80336625]\n",
      " [4.98912875]]\n",
      "  Cost: 379272.1650279643\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[-37.65510607]\n",
      " [-38.77931054]]\n",
      "  Cost: 29400956.5821305\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[336.02867494]\n",
      " [346.75054537]]\n",
      "  Cost: 2279338809.6122046\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[-2954.36774567]\n",
      " [-3047.65042117]]\n",
      "  Cost: 176708227893.0204\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[26017.06619325]\n",
      " [26839.81840681]]\n",
      "  Cost: 13699498332420.346\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[-229073.36877968]\n",
      " [-236315.92897173]]\n",
      "  Cost: 1062068568221970.8\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[2016969.77718512]\n",
      " [2080741.67012894]]\n",
      "  Cost: 8.233802554200189e+16\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[-17759193.70887963]\n",
      " [-18320696.02907109]]\n",
      "  Cost: 6.383345344176141e+18\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[1.56367755e+08]\n",
      " [1.61311724e+08]]\n",
      "  Cost: 4.948758184908083e+20\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[-1.37680091e+09]\n",
      " [-1.42033201e+09]]\n",
      "  Cost: 3.836578823835433e+22\n",
      "------------------------------\n",
      "Training RMSE: 195871866888.4185\n",
      "Testing RMSE:  196065096548.7153\n",
      "Testing R²:    -182333542551636705280.0000\n",
      "Explanation:\n",
      "- Training error is lower than testing error.\n",
      "- This is expected and indicates NO overfitting.\n",
      "- Model performance is acceptable.\n",
      "\n",
      "Learning Rate Observation:\n",
      "- Balanced learning rate.\n",
      "- Faster convergence with stability.\n",
      "\n",
      "--------------------------------------------------\n",
      "--- Learning Rate: 0.01 ---\n",
      "Iteration 1:\n",
      "  Weights:\n",
      " [[48.0336625]\n",
      " [49.8912875]]\n",
      "  Cost: 46073021.74687523\n",
      "------------------------------\n",
      "Iteration 2:\n",
      "  Weights:\n",
      " [[-4630.11653217]\n",
      " [-4775.97422926]]\n",
      "  Cost: 433938029947.9912\n",
      "------------------------------\n",
      "Iteration 3:\n",
      "  Weights:\n",
      " [[449369.64132167]\n",
      " [463578.26669624]]\n",
      "  Cost: 4087040753770662.5\n",
      "------------------------------\n",
      "Iteration 4:\n",
      "  Weights:\n",
      " [[-43610814.06952832]\n",
      " [-44989681.33172897]]\n",
      "  Cost: 3.8493750192503955e+19\n",
      "------------------------------\n",
      "Iteration 5:\n",
      "  Weights:\n",
      " [[4.23238165e+09]\n",
      " [4.36619927e+09]]\n",
      "  Cost: 3.6255297980961944e+23\n",
      "------------------------------\n",
      "Iteration 6:\n",
      "  Weights:\n",
      " [[-4.10747993e+11]\n",
      " [-4.23734847e+11]]\n",
      "  Cost: 3.4147014128654834e+27\n",
      "------------------------------\n",
      "Iteration 7:\n",
      "  Weights:\n",
      " [[3.98626419e+13]\n",
      " [4.11230018e+13]]\n",
      "  Cost: 3.2161329208074426e+31\n",
      "------------------------------\n",
      "Iteration 8:\n",
      "  Weights:\n",
      " [[-3.86862564e+15]\n",
      " [-3.99094218e+15]]\n",
      "  Cost: 3.0291113961913157e+35\n",
      "------------------------------\n",
      "Iteration 9:\n",
      "  Weights:\n",
      " [[3.75445872e+17]\n",
      " [3.87316558e+17]]\n",
      "  Cost: 2.852965370670217e+39\n",
      "------------------------------\n",
      "Iteration 10:\n",
      "  Weights:\n",
      " [[-3.64366097e+19]\n",
      " [-3.75886468e+19]]\n",
      "  Cost: 2.6870624224905124e+43\n",
      "------------------------------\n",
      "Training RMSE: 5183688283925367488512.0000\n",
      "Testing RMSE:  5188802047023904718848.0000\n",
      "Testing R²:    -127702735443625167006324057914385895522304.0000\n",
      "Explanation:\n",
      "- Training error is lower than testing error.\n",
      "- This is expected and indicates NO overfitting.\n",
      "- Model performance is acceptable.\n",
      "\n",
      "Learning Rate Observation:\n",
      "- Large learning rate.\n",
      "- May cause oscillation or divergence.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#TODO 11\n",
    "# 1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
    "# 2. Experiment with different value of learning rate, making it higher and lower, observe the result.\n",
    "\n",
    "\n",
    "def evaluate_findings():\n",
    "\n",
    "    print(\"\\nMODEL FINDINGS\\n\")\n",
    "\n",
    "    # Step 1: Load Dataset\n",
    "    print(\"Step 1: Loading dataset (student.csv)\")\n",
    "    data = pd.read_csv(\"student.csv\")\n",
    "\n",
    "    X = data[['Math', 'Reading']].values\n",
    "    Y = data['Writing'].values\n",
    "\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Features: Math & Reading\")\n",
    "    print(\"Target: Writing\\n\")\n",
    "\n",
    "    # Step 2: Train-Test Split\n",
    "    print(\"Step 2: Splitting dataset into Training (80%) and Testing (20%)\")\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    split = int(0.8 * len(X))\n",
    "\n",
    "    X_train, X_test = X[indices[:split]], X[indices[split:]]\n",
    "    Y_train, Y_test = Y[indices[:split]], Y[indices[split:]]\n",
    "\n",
    "    print(\"Training samples:\", len(X_train))\n",
    "    print(\"Testing samples:\", len(X_test), \"\\n\")\n",
    "\n",
    "    # Step 3: Experiment with Learning Rates\n",
    "    print(\"Step 3: Experimenting with different learning rates\")\n",
    "    print(\"Purpose: Observe convergence speed and model stability\\n\")\n",
    "\n",
    "    learning_rates = [0.00001, 0.0001, 0.001, 0.01]\n",
    "    iterations = 10\n",
    "    W_init = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for alpha in learning_rates:\n",
    "\n",
    "        print(f\"--- Learning Rate: {alpha} ---\")\n",
    "\n",
    "        # Train model\n",
    "        W_final, cost_history = gradient_descent(\n",
    "            X_train, Y_train, W_init, alpha, iterations\n",
    "        )\n",
    "\n",
    "        # Predictions\n",
    "        Y_train_pred = np.dot(X_train, W_final)\n",
    "        Y_test_pred = np.dot(X_test, W_final)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        train_rmse = rmse(Y_train, Y_train_pred)\n",
    "        test_rmse = rmse(Y_test, Y_test_pred)\n",
    "        test_r2 = r2(Y_test, Y_test_pred)\n",
    "\n",
    "        print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"Testing RMSE:  {test_rmse:.4f}\")\n",
    "        print(f\"Testing R²:    {test_r2:.4f}\")\n",
    "\n",
    "        # Step 4: Explain Model Behavior\n",
    "        if train_rmse < test_rmse:\n",
    "            print(\"Explanation:\")\n",
    "            print(\"- Training error is lower than testing error.\")\n",
    "            print(\"- This is expected and indicates NO overfitting.\")\n",
    "            print(\"- Model performance is acceptable.\\n\")\n",
    "        else:\n",
    "            print(\"Explanation:\")\n",
    "            print(\"- Training error is higher than testing error.\")\n",
    "            print(\"- This may indicate overfitting or instability.\\n\")\n",
    "\n",
    "        # Step 5: Learning Rate Interpretation\n",
    "        if alpha < 0.0001:\n",
    "            print(\"Learning Rate Observation:\")\n",
    "            print(\"- Very small learning rate.\")\n",
    "            print(\"- Converges slowly but stable.\\n\")\n",
    "        elif alpha > 0.001:\n",
    "            print(\"Learning Rate Observation:\")\n",
    "            print(\"- Large learning rate.\")\n",
    "            print(\"- May cause oscillation or divergence.\\n\")\n",
    "        else:\n",
    "            print(\"Learning Rate Observation:\")\n",
    "            print(\"- Balanced learning rate.\")\n",
    "            print(\"- Faster convergence with stability.\\n\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Execute \n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_findings()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
